{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccd1858-db5c-4afd-bd3e-cf56492383b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark.ml.decomposition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HashingTF, IDF\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark.ml.decomposition'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.decomposition import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sample Spark session\n",
    "spark = SparkSession.builder.appName(\"LSIExample\").getOrCreate()\n",
    "\n",
    "# Sample documents\n",
    "data = spark.createDataFrame([\n",
    "(0, \"I love machine learning\"),\n",
    "(1, \"Machine learning is fun\"),\n",
    "(2, \"Natural language processing is awesome\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Tokenize and convert text into features\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Pipeline for preprocessing\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction (LSI-like)\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(result)\n",
    "lsi_result = pca_model.transform(result)\n",
    "\n",
    "lsi_result.select(\"id\", \"pcaFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72549df8-8955-4b55-867a-ccf57c3d93ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sample documents\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      7\u001b[0m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love machine learning\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine learning is fun\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural language processing is awesome\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Sample documents\n",
    "data = spark.createDataFrame([\n",
    "(0, \"I love machine learning\"),\n",
    "(1, \"Machine learning is fun\"),\n",
    "(2, \"Natural language processing is awesome\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# Convert words to term frequency vectors\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "\n",
    "# LDA model to find topics\n",
    "lda = LDA(k=2, seed=1, optimizer=\"em\", featuresCol=\"rawFeatures\",\n",
    "topicDistributionCol=\"topicDistribution\")\n",
    "\n",
    "# Set up pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lda])\n",
    "\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)\n",
    "\n",
    "# Show the topics in the result\n",
    "result.select(\"id\", \"topicDistribution\").show()\n",
    "\n",
    "# Get the topics\n",
    "topics = model.stages[-1].describeTopics(5)\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2c76b2-e359-4718-896c-2613367883f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sample labeled data (id, text, label)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love machine learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      9\u001b[0m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine learning is fun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     10\u001b[0m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural language processing is awesome\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenize text and convert to term frequency vectors\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Sample labeled data (id, text, label)\n",
    "data = spark.createDataFrame([\n",
    "\n",
    "(0, \"I love machine learning\", 0),\n",
    "(1, \"Machine learning is fun\", 0),\n",
    "(2, \"Natural language processing is awesome\", 1)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Tokenize text and convert to term frequency vectors\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Naive Bayes classifier\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)\n",
    "\n",
    "# Show predictions\n",
    "result.select(\"id\", \"text\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b4916-1489-44ac-95df-d47bb4f97a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
