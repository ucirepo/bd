{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS9l6m1NfMkZ",
        "outputId": "598d132a-5a31-4909-e2af-4642c86041ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|  Bob|\n",
            "|Carol|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"EmployeesSQL\").getOrCreate()\n",
        "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "result = spark.sql(\"SELECT name FROM employees WHERE salary > 55000\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"employees.csv\")\n",
        "header = rdd.first()\n",
        "data_rdd = rdd.filter(lambda line: line != header)\n",
        "parsed_rdd = data_rdd.map(lambda line: line.split(\",\")).map(lambda x: (x[1], int(x[3])))\n",
        "filtered = parsed_rdd.filter(lambda x: x[1] > 55000)\n",
        "filtered.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2UV_nGdfcnj",
        "outputId": "1af707cf-ac30-4b7b-9259-87ce9db5f03e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Bob', 60000), ('Carol', 65000)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "df = spark.read.csv(\"experience_salary.csv\", header=True, inferSchema=True)\n",
        "vec_assembler = VectorAssembler(inputCols=[\"years_experience\"], outputCol=\"features\")\n",
        "data = vec_assembler.transform(df)\n",
        "lr = LinearRegression(featuresCol='features', labelCol='salary')\n",
        "model = lr.fit(data)\n",
        "print(\"Coefficients:\", model.coefficients)\n",
        "print(\"Intercept:\", model.intercept)\n",
        "\n",
        "new_data_point = spark.createDataFrame([[8]], [\"years_experience\"])\n",
        "\n",
        "predicted_data_with_features = vec_assembler.transform(new_data_point)\n",
        "\n",
        "pred_result = model.transform(predicted_data_with_features)\n",
        "\n",
        "pred_result.select(\"prediction\").show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G_BHeT1g1kL",
        "outputId": "583a831c-1e0a-47f3-9169-e1af848ff623"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [5142.8571428571395]\n",
            "Intercept: 34142.85714285715\n",
            "+-----------------+\n",
            "|       prediction|\n",
            "+-----------------+\n",
            "|75285.71428571426|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "df = spark.read.csv(\"svm_data.csv\", header=True, inferSchema=True)\n",
        "vec_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "data = vec_assembler.transform(df)\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = svm.fit(data)\n",
        "print(\"Coefficients:\", model.coefficients)\n",
        "print(\"Intercept:\", model.intercept)\n",
        "predictions = model.transform(data)\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVRPVZGOfhPx",
        "outputId": "583e16a1-cd73-460f-87ac-b04d2ed619a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [1.1144812683762066,1.5208462218869392]\n",
            "Intercept: -5.6310353600381875\n",
            "+---------+-----+----------+\n",
            "| features|label|prediction|\n",
            "+---------+-----+----------+\n",
            "|[0.5,1.0]|    0|       0.0|\n",
            "|[1.5,1.8]|    0|       0.0|\n",
            "|[1.0,2.0]|    0|       0.0|\n",
            "|[3.0,3.5]|    1|       1.0|\n",
            "|[2.0,3.0]|    1|       1.0|\n",
            "|[2.5,3.2]|    1|       1.0|\n",
            "|[3.5,4.0]|    1|       1.0|\n",
            "|[0.2,0.5]|    0|       0.0|\n",
            "+---------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "df = spark.read.csv(\"points.csv\", header=True, inferSchema=True)\n",
        "vec_assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
        "data = vec_assembler.transform(df)\n",
        "kmeans = KMeans(k=2, seed=1)\n",
        "model = kmeans.fit(data)\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers:\", centers)\n",
        "predictions = model.transform(data)\n",
        "predictions.select(\"x\", \"y\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaL13LBXfjO1",
        "outputId": "aa17458f-24fc-4e37-b3c5-b42368af848d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([3.9, 5.1]), array([1.25, 1.5 ])]\n",
            "+---+---+----------+\n",
            "|  x|  y|prediction|\n",
            "+---+---+----------+\n",
            "|1.0|1.0|         1|\n",
            "|1.5|2.0|         1|\n",
            "|3.0|4.0|         0|\n",
            "|5.0|7.0|         0|\n",
            "|3.5|5.0|         0|\n",
            "|4.5|5.0|         0|\n",
            "|3.5|4.5|         0|\n",
            "+---+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "df = spark.read.text(\"lsi_docs.txt\").toDF(\"text\")\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "words_data = tokenizer.transform(df)\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
        "model = cv.fit(words_data)\n",
        "result = model.transform(words_data)\n",
        "rows = result.select(\"features\").rdd.map(lambda x: x[0].toArray())\n",
        "row_matrix = RowMatrix(rows)\n",
        "svd = row_matrix.computeSVD(2, computeU=True)\n",
        "print(\"Singular Values:\", svd.s)\n",
        "print(\"First 2 rows of U:\")\n",
        "svd.U.rows.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDmKo7c2hIXQ",
        "outputId": "f33e49fe-4295-44d4-fbf3-97b4e8a56cdb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Singular Values: [4.344585204685772,3.022309029060833]\n",
            "First 2 rows of U:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DenseVector([0.555, -0.1307]), DenseVector([0.5909, 0.073])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyNadf1UhMDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}